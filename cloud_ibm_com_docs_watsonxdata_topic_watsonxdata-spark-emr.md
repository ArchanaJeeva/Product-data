IBM Cloud Docs Skip to contentIBM CloudCatalogCatalogCost EstimatorCost EstimatorHelpHelpDocsLog inSign upError Skip to content IBM Cloud IBM Cloud CatalogCatalog CatalogCatalog Cost EstimatorCost Estimator Cost EstimatorCost Estimator HelpHelpDocs HelpHelp Docs Docs Docs Log in Log in Sign up Sign up Error Using AWS EMR for Spark use case Using AWS EMR for Spark use case The topic provides the procedure to run Spark applications from Amazon Web Services Elastic MapReduce (AWS EMR) to achieve the IBMÂ® watsonx.data Spark use cases: data ingestion data querying table maintenance Prerequisites Provision IBMÂ® watsonx.data instance. Create a catalog with S3 bucket. Get S3 bucket credentials. Set up EMR cluster on AWS. For more information, see Setting up an EMR Cluster. Fetch the following information from IBMÂ® watsonx.data: HMS URL from watsonx.data. For more information about getting the HMS credentials, see Getting (Hive metastore) HMS Credentials. HMS Credentials from watsonx.data. For more information about getting the HMS credentials, see Getting (Hive metastore) HMS Credentials. Overview To work with source data that resides in AWS S3 buckets, you can do either of the following ways: setup watsonx.data instance on AWS configure IBM Cloud based watsonx.data instance and include an AWS S3 bucket-based catalog. The watsonx.data query engines can run queries on data from AWS S3 buckets. In both cases, you can run the data ingestion and Iceberg-based schema maintenance operations by using AWS EMR Spark. About the sample use case The sample python file (amazon-lakehouse.py) demonstrates creating schema (amazonschema), tables, and ingesting data. It also supports table maintenance operations. For more information about the functionalities in the sample, see About the sample use case Running the sample use case Follow the steps to run the Spark sample python file. Connect to the AWS EMR cluster. For more information about using SSH to connect to the EMR cluster, see Setting up EMR Cluster. Save the following sample python file. Spark sample python file from pyspark.sql import SparkSession import os def init\_spark(): spark = SparkSession.builder.appName("lh-hms-cloud")\ .enableHiveSupport().getOrCreate() return spark def create\_database(spark): # Create a database in the lakehouse catalog spark.sql("create database if not exists lakehouse.amazonschema LOCATION 's3a://lakehouse-bucket-amz/'") def list\_databases(spark): # list the database under lakehouse catalog spark.sql("show databases from lakehouse").show() def basic\_iceberg\_table\_operations(spark): # demonstration: Create a basic Iceberg table, insert some data and then query table spark.sql("create table if not exists lakehouse.amazonschema.testTable(id INTEGER, name VARCHAR(10), age INTEGER, salary DECIMAL(10, 2)) using iceberg").show() spark.sql("insert into lakehouse.amazonschema.testTable values(1,'Alan',23,3400.00),(2,'Ben',30,5500.00), (3,'Chen',35,6500.00)") spark.sql("select \* from lakehouse.amazonschema.testTable").show() def create\_table\_from\_parquet\_data(spark): # load parquet data into dataframce df = spark.read.option("header",True).parquet("s3a://source-bucket-amz/nyc-taxi/yellow\_tripdata\_2022- 01.parquet") # write the dataframe into an Iceberg table df.writeTo("lakehouse.amazonschema.yellow\_taxi\_2022").create() # describe the table created spark.sql('describe table lakehouse.amazonschema.yellow\_taxi\_2022').show(25) # query the table spark.sql('select \* from lakehouse.amazonschema.yellow\_taxi\_2022').count() def ingest\_from\_csv\_temp\_table(spark): # load csv data into a dataframe csvDF = spark.read.option("header",True).csv("s3a://source-bucket-amz/zipcodes.csv") csvDF.createOrReplaceTempView("tempCSVTable") # load temporary table into an Iceberg table spark.sql('create or replace table lakehouse.amazonschema.zipcodes using iceberg as select \* from tempCSVTable') # describe the table created spark.sql('describe table lakehouse.amazonschema.zipcodes').show(25) # query the table spark.sql('select \* from lakehouse.amazonschema.zipcodes').show() def ingest\_monthly\_data(spark): df\_feb = spark.read.option("header",True).parquet("s3a://source-bucket-amz//nyc-taxi/yellow\_tripdata\_2022-02.parquet") df\_march = spark.read.option("header",True).parquet("s3a://source-bucket-amz//nyc-taxi/yellow\_tripdata\_2022-03.parquet") df\_april = spark.read.option("header",True).parquet("s3a://source-bucket-amz//nyc-taxi/yellow\_tripdata\_2022-04.parquet") df\_may = spark.read.option("header",True).parquet("s3a://source-bucket-amz//nyc-taxi/yellow\_tripdata\_2022-05.parquet") df\_june = spark.read.option("header",True).parquet("s3a://source-bucket-amz//nyc-taxi/yellow\_tripdata\_2022-06.parquet") df\_q1\_q2 = df\_feb.union(df\_march).union(df\_april).union(df\_may).union(df\_june) df\_q1\_q2.write.insertInto("lakehouse.amazonschema.yellow\_taxi\_2022") def perform\_table\_maintenance\_operations(spark): # Query the metadata files table to list underlying data files spark.sql("SELECT file\_path, file\_size\_in\_bytes FROM lakehouse.amazonschema.yellow\_taxi\_2022.files").show() # There are many smaller files compact them into files of 200MB each using the # `rewrite\_data\_files` Iceberg Spark procedure spark.sql(f"CALL lakehouse.system.rewrite\_data\_files(table => 'amazonschema.yellow\_taxi\_2022', options => map('target-file-size-bytes','209715200'))").show() # Again, query the metadata files table to list underlying data files; 6 files are compacted # to 3 files spark.sql("SELECT file\_path, file\_size\_in\_bytes FROM lakehouse.amazonschema.yellow\_taxi\_2022.files").show() # List all the snapshots # Expire earlier snapshots. Only latest one with comacted data is required # Again, List all the snapshots to see only 1 left spark.sql("SELECT committed\_at, snapshot\_id, operation FROM lakehouse.amazonschema.yellow\_taxi\_2022.snapshots").show() #retain only the latest one latest\_snapshot\_committed\_at = spark.sql("SELECT committed\_at, snapshot\_id, operation FROM lakehouse.amazonschema.yellow\_taxi\_2022.snapshots").tail(1)[0].committed\_at print (latest\_snapshot\_committed\_at) spark.sql(f"CALL lakehouse.system.expire\_snapshots(table => 'amazonschema.yellow\_taxi\_2022',older\_than => TIMESTAMP '{latest\_snapshot\_committed\_at}',retain\_last => 1)").show() spark.sql("SELECT committed\_at, snapshot\_id, operation FROM lakehouse.amazonschema.yellow\_taxi\_2022.snapshots").show() # Removing Orphan data files spark.sql(f"CALL lakehouse.system.remove\_orphan\_files(table => 'amazonschema.yellow\_taxi\_2022')").show(truncate=False) # Rewriting Manifest Files spark.sql(f"CALL lakehouse.system.rewrite\_manifests('amazonschema.yellow\_taxi\_2022')").show() def evolve\_schema(spark): # demonstration: Schema evolution # Add column fare\_per\_mile to the table spark.sql('ALTER TABLE lakehouse.amazonschema.yellow\_taxi\_2022 ADD COLUMN(fare\_per\_mile double)') # describe the table spark.sql('describe table lakehouse.amazonschema.yellow\_taxi\_2022').show(25) def clean\_database(spark): # clean-up the demo database spark.sql('drop table if exists lakehouse.amazonschema.testTable purge') spark.sql('drop table if exists lakehouse.amazonschema.zipcodes purge') spark.sql('drop table if exists lakehouse.amazonschema.yellow\_taxi\_2022 purge') spark.sql('drop database if exists lakehouse.amazonschema cascade') def main(): try: spark = init\_spark() clean\_database(spark) create\_database(spark) list\_databases(spark) basic\_iceberg\_table\_operations(spark) # demonstration: Ingest parquet and csv data into a wastonx.data Iceberg table create\_table\_from\_parquet\_data(spark) ingest\_from\_csv\_temp\_table(spark) # load data for the month of Feburary to June into the table yellow\_taxi\_2022 created above ingest\_monthly\_data(spark) # demonstration: Table maintenance perform\_table\_maintenance\_operations(spark) # demonstration: Schema evolution evolve\_schema(spark) finally: # clean-up the demo database #clean\_database(spark) spark.stop() if \_\_name\_\_ == '\_\_main\_\_': main() Run the following commands to download the Hive metastore JAR file from the location to your workstation: The JAR file must be present in the /home/hadoop location on all nodes of the cluster. Make a note of the spark.driver.extraClassPath and spark.executor.extraClassPath. wget https://github.com/IBM-Cloud/IBM-Analytics-Engine/raw/master/wxd- connectors/hms-connector/hive-exec-2.3.9-core.jar wget https://github.com/IBM-Cloud/IBM-Analytics-Engine/raw/master/wxd-connectors/hms- connector/hive-common-2.3.9.jar wget https://github.com/IBM-Cloud/IBM-Analytics-Engine/raw/master/wxd-connectors/hms-connector/hive- metastore-2.3.9.jar Configure the HMS connection details in the AWS EMR cluster to connect to the watsonx.data Hive Meta Store (HMS). A sample command to use spark-submit from an EMR-6.12.0 (Spark 3.4.1) based cluster is as follows: Run the command from EMR on the EC2 cluster to submit the Sample spark job. spark-submit \ --deploy-mode cluster \ --jars https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.4\_2.12/1.4.0/iceberg-spark-runtime-3.4\_2.12- 1.4.0.jar,/usr/lib/hadoop/hadoop-aws.jar,/usr/share/aws/aws-java-sdk/aws-java-sdk-bundle\*.jar,/usr/lib/hadoop-lzo/lib/\* \ --conf spark.sql.catalogImplementation=hive \ --conf spark.driver.extraClassPath=/home/hadoop/hive-common-2.3.9.jar:/home/hadoop/hive-metastore- 2.3.9.jar:/home/hadoop/hive-exec-2.3.9-core.jar \ --conf spark.executor.extraClassPath=/home/hadoop/hive-common- 2.3.9.jar:/home/hadoop/hive-metastore-2.3.9.jar:/home/hadoop/hive-exec-2.3.9-core.jar \ --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \ --conf spark.sql.iceberg.vectorization.enabled=false \ --conf spark.sql.catalog.lakehouse=org.apache.iceberg.spark.SparkCatalog \ --conf spark.sql.catalog.lakehouse.type=hive \ --conf spark.hive.metastore.uris==<> \ --conf spark.hive.metastore.client.auth.mode=PLAIN \ --conf spark.hive.metastore.client.plain.username=ibmlhapikey \ --conf spark.hive.metastore.client.plain.password=<> \ --conf spark.hive.metastore.use.SSL=true \ --conf spark.hive.metastore.truststore.type=JKS \ --conf spark.hive.metastore.truststore.path=file:///etc/pki/java/cacerts \ --conf spark.hive.metastore.truststore.password=changeit \ amazon-lakehouse.py Parameter values: <> : The Hive metastore URI endpoint to access the metastore. For more information on getting the HMS credentials, see Getting (Hive metastore) HMS Credentials. <> : The password to access the metastore. For more information on getting the HMS credentials, see Getting (Hive metastore) HMS Credentials. To run the Spark python file using EMR-6.11.1 (Spark 3.3.2) cluster, download the iceberg jars from the location and follow the same procedure. Using AWS EMR for Spark use case The topic provides the procedure to run Spark applications from Amazon Web Services Elastic MapReduce (AWS EMR) to achieve the IBMÂ® watsonx.data Spark use cases: data ingestion data querying table maintenance Prerequisites Provision IBMÂ® watsonx.data instance. Create a catalog with S3 bucket. Get S3 bucket credentials. Set up EMR cluster on AWS. For more information, see Setting up an EMR Cluster. Setting up an EMR Cluster Fetch the following information from IBMÂ® watsonx.data: HMS URL from watsonx.data. For more information about getting the HMS credentials, see Getting (Hive metastore) HMS Credentials. HMS Credentials from watsonx.data. For more information about getting the HMS credentials, see Getting (Hive metastore) HMS Credentials. HMS URL from watsonx.data. For more information about getting the HMS credentials, see Getting (Hive metastore) HMS Credentials. Getting (Hive metastore) HMS Credentials HMS Credentials from watsonx.data. For more information about getting the HMS credentials, see Getting (Hive metastore) HMS Credentials. Getting (Hive metastore) HMS Credentials Overview To work with source data that resides in AWS S3 buckets, you can do either of the following ways: setup watsonx.data instance on AWS configure IBM Cloud based watsonx.data instance and include an AWS S3 bucket-based catalog. The watsonx.data query engines can run queries on data from AWS S3 buckets. In both cases, you can run the data ingestion and Iceberg-based schema maintenance operations by using AWS EMR Spark. About the sample use case The sample python file (amazon-lakehouse.py) demonstrates creating schema (amazonschema), tables, and ingesting data. It also supports table maintenance operations. For more information about the functionalities in the sample, see About the sample use case About the sample use case Running the sample use case Follow the steps to run the Spark sample python file. Connect to the AWS EMR cluster. For more information about using SSH to connect to the EMR cluster, see Setting up EMR Cluster. Connect to the AWS EMR cluster. For more information about using SSH to connect to the EMR cluster, see Setting up EMR Cluster. Setting up EMR Cluster Save the following sample python file. Spark sample python file from pyspark.sql import SparkSession import os def init\_spark(): spark = SparkSession.builder.appName("lh-hms-cloud")\ .enableHiveSupport().getOrCreate() return spark def create\_database(spark): # Create a database in the lakehouse catalog spark.sql("create database if not exists lakehouse.amazonschema LOCATION 's3a://lakehouse-bucket-amz/'") def list\_databases(spark): # list the database under lakehouse catalog spark.sql("show databases from lakehouse").show() def basic\_iceberg\_table\_operations(spark): # demonstration: Create a basic Iceberg table, insert some data and then query table spark.sql("create table if not exists lakehouse.amazonschema.testTable(id INTEGER, name VARCHAR(10), age INTEGER, salary DECIMAL(10, 2)) using iceberg").show() spark.sql("insert into lakehouse.amazonschema.testTable values(1,'Alan',23,3400.00),(2,'Ben',30,5500.00), (3,'Chen',35,6500.00)") spark.sql("select \* from lakehouse.amazonschema.testTable").show() def create\_table\_from\_parquet\_data(spark): # load parquet data into dataframce df = spark.read.option("header",True).parquet("s3a://source-bucket-amz/nyc-taxi/yellow\_tripdata\_2022- 01.parquet") # write the dataframe into an Iceberg table df.writeTo("lakehouse.amazonschema.yellow\_taxi\_2022").create() # describe the table created spark.sql('describe table lakehouse.amazonschema.yellow\_taxi\_2022').show(25) # query the table spark.sql('select \* from lakehouse.amazonschema.yellow\_taxi\_2022').count() def ingest\_from\_csv\_temp\_table(spark): # load csv data into a dataframe csvDF = spark.read.option("header",True).csv("s3a://source-bucket-amz/zipcodes.csv") csvDF.createOrReplaceTempView("tempCSVTable") # load temporary table into an Iceberg table spark.sql('create or replace table lakehouse.amazonschema.zipcodes using iceberg as select \* from tempCSVTable') # describe the table created spark.sql('describe table lakehouse.amazonschema.zipcodes').show(25) # query the table spark.sql('select \* from lakehouse.amazonschema.zipcodes').show() def ingest\_monthly\_data(spark): df\_feb = spark.read.option("header",True).parquet("s3a://source-bucket-amz//nyc-taxi/yellow\_tripdata\_2022-02.parquet") df\_march = spark.read.option("header",True).parquet("s3a://source-bucket-amz//nyc-taxi/yellow\_tripdata\_2022-03.parquet") df\_april = spark.read.option("header",True).parquet("s3a://source-bucket-amz//nyc-taxi/yellow\_tripdata\_2022-04.parquet") df\_may = spark.read.option("header",True).parquet("s3a://source-bucket-amz//nyc-taxi/yellow\_tripdata\_2022-05.parquet") df\_june = spark.read.option("header",True).parquet("s3a://source-bucket-amz//nyc-taxi/yellow\_tripdata\_2022-06.parquet") df\_q1\_q2 = df\_feb.union(df\_march).union(df\_april).union(df\_may).union(df\_june) df\_q1\_q2.write.insertInto("lakehouse.amazonschema.yellow\_taxi\_2022") def perform\_table\_maintenance\_operations(spark): # Query the metadata files table to list underlying data files spark.sql("SELECT file\_path, file\_size\_in\_bytes FROM lakehouse.amazonschema.yellow\_taxi\_2022.files").show() # There are many smaller files compact them into files of 200MB each using the # `rewrite\_data\_files` Iceberg Spark procedure spark.sql(f"CALL lakehouse.system.rewrite\_data\_files(table => 'amazonschema.yellow\_taxi\_2022', options => map('target-file-size-bytes','209715200'))").show() # Again, query the metadata files table to list underlying data files; 6 files are compacted # to 3 files spark.sql("SELECT file\_path, file\_size\_in\_bytes FROM lakehouse.amazonschema.yellow\_taxi\_2022.files").show() # List all the snapshots # Expire earlier snapshots. Only latest one with comacted data is required # Again, List all the snapshots to see only 1 left spark.sql("SELECT committed\_at, snapshot\_id, operation FROM lakehouse.amazonschema.yellow\_taxi\_2022.snapshots").show() #retain only the latest one latest\_snapshot\_committed\_at = spark.sql("SELECT committed\_at, snapshot\_id, operation FROM lakehouse.amazonschema.yellow\_taxi\_2022.snapshots").tail(1)[0].committed\_at print
