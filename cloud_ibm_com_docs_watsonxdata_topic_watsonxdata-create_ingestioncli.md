IBM Cloud Docs Skip to contentIBM CloudCatalogCatalogCost EstimatorCost EstimatorHelpHelpDocsLog inSign upError Skip to content IBM Cloud IBM Cloud CatalogCatalog CatalogCatalog Cost EstimatorCost Estimator Cost EstimatorCost Estimator HelpHelpDocs HelpHelp Docs Docs Docs Log in Log in Sign up Sign up Error Command line ingestion in Presto ingestion mode Command line ingestion in Presto ingestion mode You can run the ibm-lh tool to ingest data into IBMÂ® watsonx.data through the command line interface (CLI). This topic provides details of using the command line for ingestion in the Presto ingestion mode. Before you begin Set the mandatory environment variable ENABLED\_INGEST\_MODE to PRESTO before starting an ingestion job by running the following command: export ENABLED\_INGEST\_MODE=PRESTO Set the environment variables for SOURCE\_S3\_CREDS and STAGING\_S3\_CREDS based on the requirements before starting an ingestion job by running the following commands:

export SOURCE\_S3\_CREDS="AWS\_ACCESS\_KEY\_ID=,AWS\_SECRET\_ACCESS\_KEY=,ENDPOINT\_URL=,AWS\_REGION=,BUCKET\_NAME=" export STAGING\_S3\_CREDS="AWS\_ACCESS\_KEY\_ID=,AWS\_SECRET\_ACCESS\_KEY=,ENDPOINT\_URL=,AWS\_REGION=,BUCKET\_NAME=" About this task The commands must be run within the ibm-lh container. For more details and instructions to install ibm-lh-client package and use the ibm-lh tool for ingestion, see Installing ibm-lh-client and Setting up the ibm-lh command-line utility. To access IBM Cloud Object Storage (COS) and MinIO object storage, specify the ENDPOINT\_URL to pass the corresponding url to the tool. For more information about IBM COS, see Endpoints and storage locations. Replace the absolute values in the command examples with the values applicable to your environment. See Options and variables supported in ibm-lh tool. Following are the details of the command line option to ingest data files from S3 or local location to watsonx.data Iceberg table, in Presto ingestion mode: Ingest a single CSV/Parquet file from S3 location by using command To ingest a single CSV/Parquet file from an S3 location, run the following command: ibm-lh data-copy --source-data-files SOURCE\_DATA\_FILE \ --staging-location s3://lh-target/staging \ --target-table TARGET\_TABLES \ --ingestion-engine-endpoint INGESTION\_ENGINE\_ENDPOINT \ --dbuser DBUSER \ --dbpassword DBPASSWORD \ -- create-if-not-exist For example: ibm-lh data-copy --source-data-files s3://cust-bucket/warehouse/a\_source\_file.parquet \ --staging-location s3://cust- bucket/warehouse/staging/ \ --target-table iceberg\_target\_catalog.ice\_schema.cust\_tab1 \ --ingestion-engine-endpoint "hostname=localhost,port=8080" \ - -create-if-not-exist Ingest multiple CSV/Parquet files and CSV folders from S3 location by using a command To ingest multiple CSV/Parquet files and

CSV folders from an S3 location, run the following command: ibm-lh data-copy --source-data-files SOURCE\_DATA\_FILE \ --staging-location s3://lh- target/staging \ --target-table TARGET\_TABLES \ --ingestion-engine-endpoint INGESTION\_ENGINE\_ENDPOINT \ --dbuser DBUSER \ -- dbpassword DBPASSWORD \ --create-if-not-exist For example: ibm-lh data-copy --source-data-files s3://cust- bucket/warehouse/a\_source\_file1.csv,s3://cust-bucket/warehouse/a\_source\_file2.csv \ --staging-location s3://cust-bucket/warehouse/staging/ \ --target- table iceberg\_target\_catalog.ice\_schema.cust\_tab1 \ --ingestion-engine-endpoint "hostname=localhost,port=8080" \ --create-if-not-exist ibm-lh data-copy --source-data-files s3://cust-bucket/warehouse/ \ --staging-location s3://cust-bucket/warehouse/staging/ \ --target-table iceberg\_target\_catalog.ice\_schema.cust\_tab1 \ --ingestion-engine-endpoint "hostname=localhost,port=8080" \ --create-if-not-exist Ingest all Parquet files

in a folder from S3 location by using a command To ingest all Parquet files in a folder from an S3 location, run the following command: ibm-lh data-copy -- source-data-files SOURCE\_DATA\_FILE \ --target-table TARGET\_TABLES \ --ingestion-engine-endpoint INGESTION\_ENGINE\_ENDPOINT \ -- dbuser DBUSER \ --dbpassword DBPASSWORD \ --create-if-not-exist For example: ibm-lh data-copy --source-data-files s3://cust-bucket/warehouse/

\ --target-table iceberg\_target\_catalog.ice\_schema.cust\_tab1 \ --ingestion-engine-endpoint "hostname=localhost,port=8080" \ --create-if-not-exist In general, this option does not require a staging location. However, a staging location must be specified in some exceptional scenarios. When the staging location is not used, make sure that the Hive catalog configured with Presto can be used with source-data-files location. The following are the exceptional cases where a staging location is required: Any or all Parquet files in the folder are huge. Any or all Parquet files in the folder have special columns, such as TIME. Ingest a CSV/Parquet file or folder from a local file system by using command To ingest a single Parquet file from a local location, run the following command: ibm-lh data-copy --source-data-files SOURCE\_DATA\_FILE \ --staging-location s3://lh-target/staging \ --target-table TARGET\_TABLES \ -- ingestion-engine-endpoint INGESTION\_ENGINE\_ENDPOINT \ --dbuser DBUSER \ --dbpassword DBPASSWORD \ --create-if-not-exist For example: ibm-lh data-copy --source-data-files /cust-bucket/warehouse/a\_source\_file1.parquet \ --staging-location s3://cust-bucket/warehouse/staging/ \ -- target-table iceberg\_target\_catalog.ice\_schema.cust\_tab1 \ --ingestion-engine-endpoint "hostname=localhost,port=8080" \ --create-if-not-exist ibm-lh data-copy --source-data-files /cust-bucket/warehouse/ \ --staging-location s3://cust-bucket/warehouse/staging/ \ --target-table iceberg\_target\_catalog.ice\_schema.cust\_tab1 \ --ingestion-engine-endpoint "hostname=localhost,port=8080" \ --create-if-not-exist Ingest any data file

from local file system by using a command To ingest any data file from a local location, run the following command: To ingest any type of data files from a local file system, data files must be copied to ~ /ibm-lh-client/localstorage/volumes/ibm-lh directory. Now, you can access data files from /ibmlhdata/ directory by using the ibm-lh data-copy command. ibm-lh data-copy --source-data-files SOURCE\_DATA\_FILE \" \ --staging-location s3://lh- target/staging \ --target-table TARGET\_TABLES \ --staging-hive-catalog \ --schema \ --ingestion-engine-endpoint INGESTION\_ENGINE\_ENDPOINT \ --trust-store-path \ --trust-store-password \ --dbuser DBUSER \ --dbpassword DBPASSWORD \ --create-if-not-exist For example: ibm-lh data-copy --source-data-files /ibmlhdata/reptile.csv \ --staging-location s3://watsonx.data/staging \ --target-table iceberg\_data.ivt\_sanity\_test\_1.reptile \ --staging- hive-catalog hive\_test \ --schema /ibmlhdata/schema.cfg \ --ingestion-engine-endpoint "hostname=ibm-lh-lakehouse-presto-01-presto-svc-cpd- instance.apps.ivt384.cp.fyre.ibm.com,port=443" \ --trust-store-path /mnt/infra/tls/aliases/ibm-lh-lakehouse-presto-01-presto-svc-cpd- instance.apps.ivt384.cp.fyre.ibm.com:443.crt \ --trust-store-password changeit \ --dbuser xxxx\ --dbpassword xxxx \ --create-if-not-exist Ingest CSV or local Parquet or S3 Parquet files that use staging location To ingest CSV or local or S3 Parquet files that use staging location: ibm-lh data-copy --source- data-files SOURCE\_DATA\_FILE \ --staging-location s3://lh-target/staging \ --target-table TARGET\_TABLES \ --staging-hive-catalog \ --staging-hive- schema \ --ingestion-engine-endpoint INGESTION\_ENGINE\_ENDPOINT \ --trust-store-path \ --trust-store-password \ --dbuser DBUSER \ -- dbpassword DBPASSWORD \ --create-if-not-exist For example: ibm-lh data-copy --source-data-files s3://watsonx-data-0823-2/test\_icos/GVT- DATA-C.csv \ --staging-location s3://watsonx.data-staging \ --target-table iceberg\_data.test\_iceberg.gvt\_data\_v \ --staging-hive-catalog staging\_catalog \ --staging-hive-schema staging\_schema \ --ingestion-engine-endpoint "hostname=ibm-lh-lakehouse-presto-01-presto-svc-cpd- instance.apps.ivt384.cp.fyre.ibm.com,port=443" \ --trust-store-path /mnt/infra/tls/aliases/ibm-lh-lakehouse-presto-01-presto-svc-cpd- instance.apps.ivt384.cp.fyre.ibm.com:443.crt \ --trust-store-password changeit \ --dbuser xxxx\ --dbpassword xxxx \ --create-if-not-exist Here, --staging- location is s3://watsonx.data-staging. The --staging-hive-catalog that is staging\_catalog must be associated with the bucket watsonx.data-staging.

Command line ingestion in Presto ingestion mode You can run the ibm-lh tool to ingest data into IBMÂ® watsonx.data through the command line interface (CLI). This topic provides details of using the command line for ingestion in the Presto ingestion mode. Before you begin Set the mandatory environment variable ENABLED\_INGEST\_MODE to PRESTO before starting an ingestion job by running the following command: Set the environment variables for SOURCE\_S3\_CREDS and STAGING\_S3\_CREDS based on the requirements before starting an ingestion job by running the following commands: About this task The commands must be run within the ibm-lh container. For more details and instructions to install ibm-lh-client package and use the ibm-lh tool for ingestion, see Installing ibm-lh-client and Setting up the ibm-lh command-line utility. Installing ibm-lh-client Setting up the ibm-lh command-line utility To access IBM Cloud Object Storage (COS) and MinIO object storage, specify the ENDPOINT\_URL to pass the corresponding url to the tool. For more information about IBM COS, see Endpoints and storage locations. Endpoints and storage locations Replace the absolute values in the command examples with the values applicable to your environment. See Options and variables supported in ibm-lh tool. Options and variables supported in ibm-lh tool Following are the details of the command line option to ingest data files from S3 or local location to watsonx.data Iceberg table, in Presto ingestion mode: Ingest a single CSV/Parquet file from S3 location by using command To ingest a single CSV/Parquet file from an S3 location, run the following command: For example: Ingest multiple CSV/Parquet files and CSV folders from S3 location by using a command To ingest multiple CSV/Parquet files and CSV folders from an S3 location, run the following command: For example: Ingest all Parquet files in a folder from S3 location by using a command To ingest all Parquet files in a folder from an S3 location, run the following command: For example: In general, this option does not require a staging location. However, a staging location must be specified in some exceptional scenarios. When the staging location is not used, make sure that the Hive catalog configured with Presto can be used with source-data-files location. The following are the exceptional cases where a staging location is required: Any or all Parquet files in the folder are huge. Any or all Parquet files in the folder have special columns, such as TIME. Ingest a CSV/Parquet file or folder from a local file system by using command To ingest a single Parquet file from a local location, run the following command: For example: Ingest any data file from local file system by using a command To ingest any data file from a local location, run the following command: To ingest any type of data files from a local file system, data files must be copied to ~ /ibm-lh-client/localstorage/volumes/ibm-lh directory. Now, you can access data files from /ibmlhdata/ directory by using the ibm-lh data-copy command. For example: Ingest CSV or local Parquet or S3 Parquet files that use staging location To ingest CSV or local or S3 Parquet files that use staging location: For example: Here, --staging-location is s3://watsonx.data-staging. The --staging-hive-catalog that is staging\_catalog must be associated with the bucket watsonx.data-staging.
