IBM Cloud Docs Skip to contentIBM CloudCatalogCatalogCost EstimatorCost EstimatorHelpHelpDocsLog inSign upError Skip to content IBM Cloud IBM Cloud CatalogCatalog CatalogCatalog Cost EstimatorCost Estimator Cost EstimatorCost Estimator HelpHelpDocs HelpHelp Docs Docs Docs Log in Log in Sign up Sign up Error Spark ingestion through ibm-lh tool command line Spark ingestion through ibm-lh tool command line You can run the ibm-lh tool to ingest data into IBMÂ® watsonx.data through the command line interface (CLI) using the IBM Analytics Engine (Spark). The commands to run the ingestion job are listed in this topic. Before you begin You must have the Administrator role and privileges in the catalog to do ingestion through the web console. Add and register IBM Analytics Engine (Spark). See Registering an engine. Add storage for the target catalog. See Adding a storage-catalog pair. Create schema and table in the catalog for the data to be ingested. See Creating schemas and Creating tables. Procedure Set the mandatory environment variable ENABLED\_INGEST\_MODE to SPARK\_LEGACY before starting an ingestion job by running the following command: export ENABLED\_INGEST\_MODE=SPARK\_LEGACY Set the following environment variables before starting an ingestion job by running the following commands: export IBM\_LH\_BEARER\_TOKEN= export IBM\_LH\_SPARK\_JOB\_ENDPOINT=https:///v4/analytics\_engines//spark\_applications export HMS\_CLIENT\_USER=lakehouse export HMS\_CLIENT\_PASSWORD= export SOURCE\_S3\_CREDS="AWS\_ACCESS\_KEY\_ID=\*\*\*\*\*\*\*,AWS\_SECRET\_ACCESS\_KEY=\*\*\*\*\*\*\*,ENDPOINT\_URL=,AWS\_REGION=,BUCKET\_NAME=" export STAGING\_S3\_CREDS="AWS\_ACCESS\_KEY\_ID=\*\*\*\*\*\*\*,AWS\_SECRET\_ACCESS\_KEY=\*\*\*\*\*\*\*,ENDPOINT\_URL=,AWS\_REGION=,BUCKET\_NAME=" export TARGET\_S3\_CREDS="AWS\_ACCESS\_KEY\_ID=\*\*\*\*\*\*\*,AWS\_SECRET\_ACCESS\_KEY=\*\*\*\*\*\*\*,ENDPOINT\_URL=,AWS\_REGION=,BUCKET\_NAME=" export IBM\_LH\_SPARK\_EXECUTOR\_CORES= export IBM\_LH\_SPARK\_EXECUTOR\_MEMORY= export IBM\_LH\_SPARK\_EXECUTOR\_COUNT= export IBM\_LH\_SPARK\_DRIVER\_CORES= export IBM\_LH\_SPARK\_DRIVER\_MEMORY= For IBM Cloud, the Spark driver, executor vCPU and memory combinations must be in a 1:2, 1:4, or 1:8 ratio. See Default limits and quotas for Analytics Engine instances. Table 1 Environment variable name Description IBM\_LH\_BEARER\_TOKEN Authorization bearer token. CPD: https://cloud.ibm.com/apidocs/cloud-pak-data/cloud-pak-data-4.7.0#getauthorizationtoken SaaS: https://cloud.ibm.com/docs/account? topic=account-iamtoken\_from\_apikey IBM\_LH\_SPARK\_JOB\_ENDPOINT Spark applications v4 endpoint for CPD and v3 endpoint for SaaS. Refer to Step 1 in document: https://www.ibm.com/docs/en/cloud-paks/cp-data/4.7.x?topic=administering-managing-instances to retrieve CPD Spark Endpoint To retrieve SaaS Spark Endpoint: https://cloud.ibm.com/docs/AnalyticsEngine?topic=AnalyticsEngine-retrieve-endpoints-serverless HMS\_CLIENT\_USER User for Hive Metastore client. CPD Spark implementation uses lakehouse. SaaS Spark implementation uses ibmlhapikey. HMS\_CLIENT\_PASSWORD Password for Hive Metastore client. SOURCE\_S3\_CREDS S3 credentials for the source file bucket in the format:â€œAWS\_ACCESS\_KEY\_ID=,AWS\_SECRET\_ACCESS\_KEY=,ENDPOINT\_URL=,AWS\_REGION=,BUCKET\_NAME=â€ TARGET\_S3\_CREDS S3 credentials for the target table bucket in the format: â€œAWS\_ACCESS\_KEY\_ID=,AWS\_SECRET\_ACCESS\_KEY=,ENDPOINT\_URL=,AWS\_REGION=,BUCKET\_NAME=â€ IBM\_LH\_SPARK\_EXECUTOR\_CORES Optional spark engine configuration setting for executor cores IBM\_LH\_SPARK\_EXECUTOR\_MEMORY Optional spark engine configuration setting for executor memory IBM\_LH\_SPARK\_EXECUTOR\_COUNT Optional spark engine configuration setting for executor count IBM\_LH\_SPARK\_DRIVER\_CORES Optional spark engine configuration setting for driver cores IBM\_LH\_SPARK\_DRIVER\_MEMORY Optional spark engine configuration setting for driver memory You can run ingestion jobs to ingest data in 2 ways, using a simple command line or a config file. Run the following command to ingest data from a single or multiple source data files: ibm-lh data-copy --source-data-files s3://path/to/file/or/folder \ --target-table .. \ --ingestion-engine-endpoint "hostname=,port=,type=spark" \ --trust-store-password \ --trust-store-path \ --target-catalog-uri 'thrift://' Where the parameters used are listed as follows: Table 2 Parameter Description --source-data-files Path to s3 parquet or CSV file or folder. Folder paths must end with â€œ/â€. File names are case sensitive. --target-table Target table in format ... --ingestion-engine-endpoint Ingestion engine endpoint will be in the format hostname=â€™â€™,port=â€™â€™,type=sparkâ€. Type must be set to spark. --trust- store-password Password of the truststore certificate inside the spark job pod. Current password for Spark in CPD and SaaS is changeit. --trust-store-path Path of the truststore cert inside the spark job pod. Current path of Spark in CPD and SaaS is file:///opt/ibm/jdk/lib/security/cacerts. --target-catalog-uri HMS thrift endpoint. CPD endpoint example: thrift://. is taken from the details tab of the catalog in the Infrastructure page. SaaS endpoint example: thrift://. is taken from the details tab of the catalog in the Infrastructure page. --create-if-not-exist Use this option if the target schema or table is not created. Do not use if the target schema or table is already created. --schema Use this option with value in the format path/to/csvschema/config/file. Use the path to a schema.cfg file which specifies header and delimiter values for CSV source file or folder. Run the following command to ingest data from a config file: ibm-lh data-copy --ingest-config / Where the config file has the following information: [global-ingest-

config] target-tables:..

ingestion-engine:hostname='',port='',type=spark create-if-not-exist:true/false [ingest-config1] source-files:s3://path/to/file/or/folder target-catalog-uri:thrift:// trust-store-path: trust-store-password: schema:/path/to/csvschema/config/file [Optional] The parameters used in the config file ingestion job is listed as follows: Table 3 Parameter Description source-files Path to s3 parquet or CSV file or folder. Folder paths must end with â€œ/â€ target-table Target table in format ... ingestion-engine Ingestion engine endpoint will be in the format hostname=â€™â€™, port=â€™â€™,type=sparkâ€. Type must be set to spark. trust-store-password Password of the truststore certificate inside the spark job pod. Current password for Spark in CPD and SaaS is changeit. trust-store-path Path of the truststore cert inside the spark job pod. Current path of Spark in CPD and SaaS is file:///opt/ibm/jdk/lib/security/cacerts. target-catalog-uri HMS thrift endpoint. CPD endpoint example: thrift://. is taken from the details tab of the catalog in the Infrastructure page. SaaS endpoint example: thrift://. is taken from the details tab of the catalog in the Infrastructure page. create-if-not-exist Use this option if the target

schema or table is not created. Do not use if the target schema or table is already created. schema Use this option with value in the format path/to/csvschema/config/file. Use the path to a schema.cfg file which specifies header and delimiter values for CSV source file or folder. Limitations Following are some of the limitations of Spark ingestion: Spark ingestion supports only source data files from object storage bucket. Local files are not supported. The default buckets in watsonx.data are not exposed to Spark engine. Hence, iceberg-bucket and hive-bucket are not supported for source or target table. Users can use their own MinIo or S3 compatible buckets that are exposed and accessible by Spark engine. Spark ingestion through ibm-lh tool command line You can run the ibm-lh tool to ingest data into IBMÂ® watsonx.data through the command

line interface (CLI) using the IBM Analytics Engine (Spark). The commands to run the ingestion job are listed in this topic. Before you begin You must have the Administrator role and privileges in the catalog to do ingestion through the web console. Add and register IBM Analytics Engine (Spark). See Registering an engine. Registering an engine Add storage for the target catalog. See Adding a storage-catalog pair. Adding a storage-catalog pair Create schema and table in the catalog for the data to be ingested. See Creating schemas and Creating tables. Creating schemas Creating tables Procedure Set the mandatory environment variable ENABLED\_INGEST\_MODE to SPARK\_LEGACY before starting an ingestion job by running the following command: Set the following environment variables before starting an ingestion job by running the following commands: For IBM Cloud, the Spark driver, executor vCPU and memory combinations must be in a 1:2, 1:4, or 1:8 ratio. See Default limits and quotas for Analytics Engine instances. Default limits and quotas for Analytics Engine instances IBM\_LH\_BEARER\_TOKEN Authorization bearer token. CPD: https://cloud.ibm.com/apidocs/cloud-pak-data/cloud-pak-data-4.7.0#getauthorizationtoken https://cloud.ibm.com/apidocs/cloud-pak-data/cloud-pak-data- 4.7.0#getauthorizationtoken SaaS: https://cloud.ibm.com/docs/account?topic=account-iamtoken\_from\_apikey https://cloud.ibm.com/docs/account?topic=account- iamtoken\_from\_apikey IBM\_LH\_SPARK\_JOB\_ENDPOINT Spark applications v4 endpoint for CPD and v3 endpoint for SaaS. Refer to Step 1 in document: https://www.ibm.com/docs/en/cloud-paks/cp-data/4.7.x?topic=administering-managing-instances to retrieve CPD Spark Endpoint https://www.ibm.com/docs/en/cloud- paks/cp-data/4.7.x?topic=administering-managing-instances To retrieve SaaS Spark Endpoint: https://cloud.ibm.com/docs/AnalyticsEngine?topic=AnalyticsEngine-retrieve- endpoints-serverless https://cloud.ibm.com/docs/AnalyticsEngine?topic=AnalyticsEngine-retrieve-endpoints-serverless HMS\_CLIENT\_USER User for Hive Metastore

client. CPD Spark implementation uses lakehouse. SaaS Spark implementation uses ibmlhapikey. HMS\_CLIENT\_PASSWORD Password for Hive Metastore client. SOURCE\_S3\_CREDS S3 credentials for the source file bucket in the format:â€œAWS\_ACCESS\_KEY\_ID=,AWS\_SECRET\_ACCESS\_KEY=,ENDPOINT\_URL=,AWS\_REGION=,BUCKET\_NAME=â€ TARGET\_S3\_CREDS S3 credentials for the target table bucket in the format: â€œAWS\_ACCESS\_KEY\_ID=,AWS\_SECRET\_ACCESS\_KEY=,ENDPOINT\_URL=,AWS\_REGION=,BUCKET\_NAME=â€ IBM\_LH\_SPARK\_EXECUTOR\_CORES Optional spark engine configuration setting for executor cores IBM\_LH\_SPARK\_EXECUTOR\_MEMORY Optional spark engine configuration setting for executor memory IBM\_LH\_SPARK\_EXECUTOR\_COUNT Optional spark engine configuration setting for executor count IBM\_LH\_SPARK\_DRIVER\_CORES Optional spark engine configuration setting for driver cores IBM\_LH\_SPARK\_DRIVER\_MEMORY Optional spark engine configuration setting for driver memory You can run ingestion jobs to ingest data in 2 ways, using a simple command line or a config file. Run the following command to ingest data from a single or multiple source data files: ibm-lh data-copy --source-data-files s3://path/to/file/or/folder \ --target-table ..

\ --ingestion-engine-endpoint "hostname=,port=,type=spark" \ --trust-store-password \ --trust-store-path \ --target-catalog-uri 'thrift://' Where the parameters used are listed as follows: Table 2 Parameter Description --source-data-files Path to s3 parquet or CSV file or folder. Folder paths must end with â€œ/â€. File names are case sensitive. -- target-table Target table in format ... --ingestion-engine-endpoint Ingestion engine endpoint will be in the format hostname=â€™â€™,port=â€™â€™,type=sparkâ€. Type must be set to spark. --trust-store-password Password of the truststore certificate inside the spark job pod. Current password for Spark in CPD and SaaS is changeit. -- trust-store-path Path of the truststore cert inside the spark job pod. Current path of Spark in CPD and SaaS is file:///opt/ibm/jdk/lib/security/cacerts. --target-catalog-uri HMS thrift endpoint. CPD endpoint example: thrift://. is taken from the details tab of the catalog in the Infrastructure page. SaaS endpoint example: thrift://. is taken from the details tab of the catalog in the Infrastructure page. --create-if-not-exist Use this option if the target schema or table is not created. Do not use if the target schema or table is already created. --schema Use this option with value in the format path/to/csvschema/config/file. Use the path to a schema.cfg file which specifies header and delimiter values for CSV source file or folder. Run the following command to ingest data from a config file: ibm-lh data-copy --ingest-config / Where the config file has the following information: [global-ingest-config] target-tables:..

ingestion-engine:hostname='',port='',type=spark create-if-not-exist:true/false [ingest-config1] source-files:s3://path/to/file/or/folder target-catalog-uri:thrift:// trust-store-path: trust-store-password: schema:/path/to/csvschema/config/file [Optional] The parameters used in the config file ingestion job is listed as follows: Table 3 Parameter Description source-files Path to s3 parquet or CSV file or folder. Folder paths must end with â€œ/â€ target-table Target table in format ... ingestion-engine Ingestion engine endpoint will be in the format hostname=â€™â€™, port=â€™â€™,type=sparkâ€. Type must be set to spark. trust-store-password Password of the truststore certificate inside the spark job pod. Current password for Spark in CPD and SaaS is changeit. trust-store-path Path of the truststore cert inside the spark job pod. Current path of Spark in CPD and SaaS is file:///opt/ibm/jdk/lib/security/cacerts. target-catalog-uri HMS thrift endpoint. CPD endpoint example: thrift://. is taken from the details tab of the catalog in the Infrastructure page. SaaS endpoint example: thrift://. is taken from the details tab of the catalog in the Infrastructure page. create-if-not-exist Use this option if the target schema or table is not created. Do not use if the target schema or table is already created. schema Use this option with value in the format path/to/csvschema/config/file. Use the path to a schema.cfg file which specifies header and delimiter values for CSV source file or folder. You can run ingestion jobs to ingest data in 2 ways, using a simple command line or a config file. Run the following command to ingest data from a single or multiple source data files: Where the parameters used are listed as follows: --source- data-files Path to s3 parquet or CSV file or folder. Folder paths must end with â€œ/â€. File names are case sensitive. --target-table Target table in format ... --ingestion- engine-endpoint Ingestion engine endpoint will be in the format hostname=â€™â€™,port=â€™â€™,type=sparkâ€. Type must be set to spark. --trust-store-password Password of the truststore certificate inside the spark job pod. Current password for Spark in CPD and SaaS is changeit. --trust-store-path Path of the truststore cert inside the spark job pod. Current path of Spark in CPD and SaaS is file:///opt/ibm/jdk/lib/security/cacerts. --target-catalog-uri HMS thrift endpoint. CPD endpoint example: thrift://. is taken from the details tab of the catalog in the Infrastructure page. SaaS endpoint example: thrift://. is taken from the details tab of the catalog in the Infrastructure page. -- create-if-not-exist Use this option if the target schema or table is not created. Do not use if the target schema or table is already created. --schema Use this option with value in the format path/to/csvschema/config/file. Use the path to a schema.cfg file which specifies header and delimiter values for CSV source file or folder. Run the following command to ingest data from a config file: Where the config file has the following information: The parameters used in the config file ingestion job is listed as follows: source- files Path to s3 parquet or CSV file or folder. Folder paths must end with â€œ/â€ target-table Target table in format ... ingestion-engine Ingestion engine endpoint will be in the format hostname=â€™â€™, port=â€™â€™,type=sparkâ€. Type must be set to spark. trust-store-password Password of the truststore certificate inside the spark job pod. Current password for Spark in CPD and SaaS is changeit. trust-store-path Path of the truststore cert inside the spark job pod. Current path of Spark in CPD and SaaS is file:///opt/ibm/jdk/lib/security/cacerts. target-catalog-uri HMS thrift endpoint. CPD endpoint example: thrift://. is taken from the details tab of the catalog in the Infrastructure page. SaaS endpoint example: thrift://. is taken from the details tab of the catalog in the Infrastructure page. create-if-not-exist Use this option if the target schema or table is not created. Do not use if the target schema or table is already created. schema Use this option with value in the format path/to/csvschema/config/file. Use the path to a schema.cfg file which specifies header and delimiter values for CSV source file or folder. Limitations Following are some of the limitations of Spark ingestion: Spark ingestion supports only source data files from object storage bucket. Local files are not supported. The default buckets in watsonx.data are not exposed to Spark engine. Hence, iceberg-bucket and hive-bucket are not supported for source or target table. Users can use their own MinIo or S3 compatible buckets that are exposed and accessible by Spark engine.
